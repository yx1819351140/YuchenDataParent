# 测试环境改线上环境共计修改以下几个参数
# 1.kafka.historylabel.topic = yuchen.globalevent.setting
# 2.kafka.nlp.etl.group = kafkaSettings2mongo_dws_007
# 3.jdbc.url = jdbc:mysql://192.168.12.240:3306/global_event_prod?useUnicode=true&characterEncoding=utf-8&useSSL=false
# 4.mongo.setting.dws.table = yuchen_globalevent_setting
# 5.es.nodes = 192.168.12.220,192.168.12.221,192.168.12.222,192.168.12.223
# 6.kafka.mongo.settings.dws.group = kafkaSettings2mongo_dws_007

# kafkaConfig
kafka.nlp.push.topic = test002
kafka.nlp.consumer.group = groupA
kafka.nlp.etl.group = global_event_dev
kafka.nlp.etl.topic = nlp_result_test004
kafka.co.events.etl.group = bigData01
kafka.co.events.etl.topic = global_co_events
kafka.nlp.etl.topic.test = nlp_result_test001
kafka.historylabel.topic = yuchen.globalevent.setting_test
kafka.mongo.settings.dws.group = kafkaSettings2mongo_dws_test_29
kafka.sendhistory.topic = yuchen.globalevent.history.nlp.test
kafka.old.mongo.history.push.topic = global.event.old.mongo.history
kafka.bootstrap.servers = datanode01:9092,datanode02:9092,datanode03:9092,datanode04:9092
kafka.group.id = test001
kafka.auto.offset.reset = latest
kafka.auto.offset.reset.test = earliest
kafka.queue.max.length = 200
kafka.history.queue.max.length = 28064
#kafka.history.queue.max.length = 177095
# 推送nlp kafka数量
send.kafka.num = 500

# hive中间表
# 历史数据推送表
hive.old.mongo.history.push.table = dwm.hotevent_history2kafka_temp

# MysqlConfig
jdbc.driver = com.mysql.jdbc.Driver
jdbc.url = jdbc:mysql://192.168.12.240:3306/global_event?useUnicode=true&characterEncoding=utf-8&useSSL=false
jdbc.user = root
jdbc.password = 123456
jdbc.table = setting_info

# ESConfig
es.nodes = 192.168.12.197,192.168.12.198,192.168.12.199
es.port = 9200
es.index.auto.create = true
es.nodes.wan.only = true
es.update.retry.on.conflict = 3
es.write.operation = upsert
es.result.index = hot_news_dev
es.result.event.index = hot_events_dev
es.result.co.event.index = global_co_events_test
es.result.co.event.label.a = co_events_label_a_test
es.result.co.event.label.b = co_events_label_b_test
es.read.field.as.array.include = alias
es.batch.size.bytes = 100mb
es.batch.size.entries = 100000
es.read.field.as.array.include.label.a = task_ids,custom_routine_labels,custom_labels,related_country
es.read.field.as.array.include.label.b = alias
# 推送kafka做nlp临时观测表
es.push.nlp.temp.table = part_a_temp
# 增量打标A成功的观测表
es.real.time.label.a.table = test_a
# 增量打标B成功的观测表
es.real.time.label.b.table = test_b
# 读取元数据字段 获得_score
es.read.metadata = true
es.read.metadata.field = _score

# HbaseConfig
hbase.zookeeper.quorum = datanode01,datanode02,datanode03
hbase.zookeeper.property.clientPort = 2181
# 数据推送的hbase表
hbase.dwdnews.table = dwd:global_event_hot_news
# etl存入hbase底表做数据沉淀
hbase.dwd.etl.table = dwd:global_event_etl_data
hbase.testnews.table = test:dwd_news

# 新闻发布时间取ngr_date_v1时差
time.difference = 8

# 地理信息服务redis
spark.redis.host=192.168.12.226
spark.redis.port=16279

# mongo
spark.mongodb.input.batchSize = 1000
spark.mongodb.input.uri = mongodb://192.168.12.180:28100/gdelt.hot_news_history
spark.mongodb.input.partitioner = MongoPaginateByCountPartitioner
spark.mongodb.input.partitionerOptions.partitionKey = _id
spark.mongodb.input.partitionerOptions.numberOfPartitions = 32
label.spark.mongodb.input.partitionerOptions.numberOfPartitions = 4
# 任务配置信息存储表，位于大数据集群mongo
mongo.setting.ods.database = kafka_ods
mongo.setting.dws.database = kafka_dws
mongo.setting.dws.table = yuchen_globalevent_setting_test

# 新闻etl streaming程序间隔时间
spark.streaming.time.interval = 180

# 新闻etl streaming程序间隔时间
spark.streaming.time.interval.co.events.etl = 180

# 大数据接口服务调用地址
bigData.servers.ip = 192.168.12.210
bigData.servers.port = 30023

# 后端接口服务调用地址
backend.servers.ip = 192.168.12.210
backend.servers.port = 30021


# 算法服务接口调用地址
algorithm.servers.ip = 192.168.12.224
algorithm.servers.port = 20098
